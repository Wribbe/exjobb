\documentclass[nofilelist,dvipsnames]{cslthse-msc}
% to show a list of used packages at the end of the document, delete the nofilelist option
%\documentclass{cslthse-msc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[titletoc, header, page]{appendix}
\usepackage{transparent}
\usepackage[
	backend=biber,
	style=numeric,
	sorting=none,
]{biblatex}
\addbibresource{report.bib}
\usepackage{parskip}
\usepackage{textcomp}

% used to display the used files at the end. Select nofilelist as a package option to disable this
%\listfiles % initialize

%\geometry{showframe}
%better like this?
%\student{Flavius Gruian}{Flavius.Gruian@cs.lth.se}
\student{Stefan Eng}{atn08sen@lu.se}

\thesisnumber{LU-CS-EX: 2020-XX} % Birger Swahn will provide this number to you, once the thesis is ready for publication

\title{
  Usability testing on the web; Measuring how design impacts task performance
  times.
}

%\onelinetitle
%\twolinestitle
\threelinestitle
%\fourlinestitle

%\subtitle{A {\LaTeX} class}
\company{MASSIVE}
\supervisors{
  John Deer, \href{mailto:jdeer@company.com}{\texttt{jdeer@company.com}}
}{
  Don Jeer, \href{mailto:djeer@xy.lth.se}{\texttt{djeer@xy.lth.se}}
}
\examiner{Jane Doe, \href{mailto:jane.doe@cs.lth.se}{\texttt{jane.doe@cs.lth.se}}}

\date{\today}
%\date{January 16, 2015}

\acknowledgements{
}

\theabstract{
}

\keywords{
	Usability testing,
	Web-application,
	Flask,
	HTML
}

\begin{document}
\renewcommand{\bibname}{References}

\makefrontmatter
\newcommand{\todo}[1]{\textcolor{blue}{\textbf{TODO:} #1}}
\newcommand{\todoMaybe}[1]{\textcolor{OliveGreen}{\textbf{?TODO:} #1?}}
\newcommand{\eatdot}[1]{}
\newcommand{\ctitle}[1]{\citetitle{#1}\cite{#1}}
\newcommand{\varHere}[1]{\input{vars/#1.txt}\unskip}
\newcommand{\checkTruth}[0]{\textcolor{red}{(?)} }
\newcommand{\findref}[0]{\textcolor{orange}{[!]}}%
\newcommand{\todoInsert}[1]{\textcolor{purple}{\textit{<insert #1 here>}}}%
\newcommand{\numRuns}[1]{\texttt{\#r=#1}}%

\chapter{Introduction}

  As the gaming industry continue to grow{\findref\findref} so do the reported
  number of stress-related issues reported by the people working in the
  sector{\findref\findref\findref}. The initial idea for this report came from a managers
  observation that co-workers would abandon the digital communication software
  for a more hands-on approaches, such as post-it notes on a whiteboard, when
  the pressure got to a certain point.

  Asking why, people stated that the software they were supposed to use for
  communicating and propagating the projects status throughout the team got in
  their way. Which is why they opted to use post-it notes, even though it has
  significantly worse communication bandwidth and is less accessible,
  at-least-it-works\texttrademark.

  \section[MASSIVE Entertainment | A Ubisoft studio]{MASSIVE}

  {\vspace{-0.7cm}{\hspace{1.85cm}\small MASSIVE ENTERTAINMENT | A UBISOFT STUDIO}}

    \todo{Expand section.}

  \section{Why usability testing?}

    Initially, the plan was to make interface changes to the organization-software
    itself, but a question remained, how do you prove that it actually makes a
    difference?

    \todo{Describe why usability testing, add scaling problem.}

  \section{Usability testing}

    \subsection{Introduction}

      Usability is traditionally done in person with the \textit{over the
        shoulder} method which gives a good insight of what a participant does
      during a test. Further more, if the \textit{thinking out aloud - method} is
      utilized correctly, the test-moderator should have a good insight into the
      participant thought-process during the test.

    \subsection{Evolution and state of the art}

      While effective\checkTruth this method scales poorly with a one to one ratio
      between moderator and test participant. This project investigates the
      possibility of alleviating this scale constraint by utilizing a internet
      based platform to conduct usability tests of user interfaces online.

      \todo{Expand this section.}

  \section{Running usability tests at larger scales}

    \todo{Expand this section.}

	\section{Report goals}

		\begin{enumerate}
			\item{Create a web based platform for usability testing.}
			\item{Run one or several interface tests with real users on the platform.}
			\item{
				Verify that the collected data shows a significant\checkTruth impact on the studied
				variable(s) when parameters are changed. \todoMaybe{Shorten}
			}
		\end{enumerate}

	\section{Literary scope}

		This report draws and builds on information from the fields of usability
		testing, web design and interaction design.

		Specifically, \ctitle{citeHandbookUsability} and
		\ctitle{citeUsabilityTestingEssentials}, provide a contrast between
		traditional and modern approaches to usability testing and how to perform
		them.

		\ctitle{citeDonMakeMeThink} provides a concise and interesting
		summary of no-nonsense approaches to web design from a usability perspective.
		Last but not least \ctitle{citeTheDesignOfEverydayThings} introduces
		both \textit{user-centered-} and \textit{interaction-design} together
		with the concept of \textit{affordances},

	  %Introduction to user interface design and usability testing? \\
	  %
		%Figure out how much of an impact different design aspects have on tasks
		%that require distinguishing one element from another. And doing it in a
		%decentralised manner based on web-application. \\
	  %
	  %Don't make me think -> webdesign. \\
	  %Design of everyday things -> webdesign. \\
	  %Report based on distances / colors -> webdesign. \\
	  %Usability-testing guide -> webapp suggesting + other. \\


	\chapter{Approach}

		\section{Method}

      This section expands on the methodology behind the initial concept and
      creation of the testing platform, as well as the process of generating
      and evaluating data from the usability tests performed by the participants.

      \subsection{Software development methodology}

        The goal was to adopt an agile development process{\findref} for the
        creation, evaluation and improvement of the usability testing platform.

        \begin{figure}[h!]
          \centering
          \includegraphics{figures/method.pdf}
          \caption{Concept, development, testing and improvement cycle.}
        \end{figure}

        This methodology is centered around creating a minimal
        working prototype{\findref} that is put in front of real users as soon
        as possible. The feedback data generated by the users should then be
        feed back into the design to improve the next prototype version, which
        is tested again. This cycle should then be repeated until either the
        software is satisfactory or the time is up.

%      The goal was to perform several full iteration cycles, but in the
%      end
%      While this was the goal, it only completed one whole full cycle, with
%      smaller iterations within the larger cycle. \todo{Reword}

      \subsection{Defining the initial concept}

        In order to perform usability tests that can be measured and validated,
        there needs to something for users to interact with. Since the subject
        of communication under pressure was the initial focus, the suggestion
        of helping managers reduce the stress for their team came up.

        After interviewing a couple of managers, the following ideas were
        gathered:

        \begin{itemize}
          \item{
            An easy way to see if a co-worker is assigned more work than they
            have available hours.
          }
          \item{
            Calendar overview where it is possible to determine if there are
            hot-spots where lots of results need to be produced at the same
            time.
          }
          \item{
            A concise way to identify if there are critical tasks that, if
            delayed, would delay other tasks that depend on it.
          }
          \item{
            The possibility to identify a group or teams strengths and assign
            task types accordingly.
          }
        \end{itemize}

      \subsection{Paper prototypes for a first-draft interface}

        Even though the initial interface setup was very bare-bones, it was
        still important to get it in front of users, in this case, test
        participants, as soon as possible.

        \begin{figure}[h!]
          \centering
          \begin{minipage}{.49\textwidth}
            \includegraphics[width=\linewidth]{ui11.pdf}
          \end{minipage}
          \begin{minipage}{.49\textwidth}
            \includegraphics[width=\linewidth]{ui12.pdf}
          \end{minipage}
          \begin{minipage}{.49\textwidth}
            \includegraphics[width=\linewidth]{ui13.pdf}
          \end{minipage}
          \begin{minipage}{.49\textwidth}
            \includegraphics[width=\linewidth]{ui14.pdf}
          \end{minipage}
          \caption{Interface drafts 1.1, 1.2, 1.3 and 1.4.}
        \end{figure}

        \todoMaybe{These figures should probably be under result?}

        Four mock-up interfaces were created based on \todoInsert{part about
          ui design reference} and presented to five\checkTruth colleagues for
        evaluation. The evaluation was conducted as an in-person interview
        where the interviewee were asked to voiced their thoughts out aloud.
        After the initial reaction and thought about each of the designs, the
        interviewee was asked to pick, according to them, the most suitable
        design.

        After the initial pick, the interviewee was presented with
        \todoInsert{part about ISO-standard design}, which they had to arrange
        in order of most to least important according to their own views.
        After prioritizing the different design attributes, the participant was
        again asked to pick what they felt was the most suitable design.


        \todo{Expand with information about ISO design standard}

      \subsection{Gathering relevant test data}

        There is a two-fold goal that the collected test-data needs to solve:
        \begin{itemize}
          \item{
            Generate a quantifiable value that can be tracked in order to
            evaluate the participants performance during tests.
          }
          \item{
            Aggregate participant feedback about the platform in order to
            improve the next version.
          }
        \end{itemize}

        The first problem is solved by sticking with a established tradition
        within usability testing\findref\findref, measuring completion time.
        In order to keep track of their test-runs, participant are assigned an
        anonymous id-string after acknowledging that they have read the initial
        information. The anonymous id is registered as used in the database
        and the id is stored in the participants browsers web-session.

        As a participant starts a new task, the database will register the
        start-time and link it to the aforementioned anonymous id. If the
        participant completes the task, the stop-time will be recorded in the
        same database post, and the difference between these two values will be
        used as the time for task completion.

        Acquiring feedback data is done through a post-test survey with ten set
        questions (1-5) and a free-from dialog box for additional input.
        \todo{Expand and reword last section}


      \subsection{Evaluating impact and effectiveness}

        \todo{expand this section}

%			Results by measuring the time and showing graphs + statistical grouping /
%			analysis.

		\section{Theory}

			The theory contains color-theory and the experience from managerial
			positions.

			\subsection{Information presentation and color}

			\subsection{Representative simplified task models}

			\subsection{Measuring time to task completion}

			\subsection{Threshold for failure}


		\section{Implementation}

			Writing a web-application with Flask + python + sqlite3.
			HTML5 and CSS with a dab javascript.

			\subsection{Platform software stack}

			\subsection{Interface creation}

			\subsection{Variable challenges}

      \addtocontents{toc}{\pagebreak}
	\chapter{Evaluation}

		\section{Results}

			\subsection{Participation and success ratio}

				The test-site went live 2020-01-24 and the link was initially shared
				through Facebook. On 2020-01-27 the link was shared on
				the MASSIVE internal mailing list, boosting the participation
				significantly. In a total,
				\varHere{totalParticipants} participants got past the initial information
				page over a five day period.

				\begin{figure}[h!]
					\centering
					\includegraphics{figures/participantsOverTime.pdf}
					\caption{Total registered participants over time.}
				\end{figure}

				In total \varHere{totalTests} tests were run, where
				\varHere{totalTestsCorrect} were answered correctly,
				\varHere{totalTestsUncompleted} never produced an answer, leaving
				\varHere{totalTestsIncorrect} incorrect answers. Looking only on the test
				runs without any user- or task-correlation, the chance that any given test run
				produces the correct answer is $\sim$\varHere{varTotalRatioSuccess}\%.

				\begin{figure}[h!]
					\centering
					\includegraphics{figures/runsOverTime.pdf}
					\caption{Total number of tests run overt time.}
				\end{figure}

			\subsection{Tests per user and defining outliers}

				The number of recommended test was five, which when completed, allowed
				the participants to continue to the final survey. However, there was
				nothing stopping each participant from doing more or less than the
				suggested number.

				\begin{figure}[h!]
					\centering
					\includegraphics{figures/testsPerUser.pdf}
					\caption{Participants grouped on how many test they performed.}
				\end{figure}

        \ \\
        Of the \varHere{totalParticipants} number of participant sessions started,
        \varHere{valNumAnyTestsRun}
        of them ran at least one test, which means \varHere{valTestNoTests} ran
        no tests.
        %Further more, \varHere{valTestsFiveToTen} completed between
        %five and nine tests, and \varHere{valTestsElevenOrMore} completed ten or
        %more. (\varHere{valTestTenOrLessP}) (\varHere{valTestsElevenOrMoreP})

        \begin{figure}
          \centering
          \varHere{tablePrecentageOfUsers}
          \caption{Tabulated values of test-run groups and corresponding
            percentage of total participants (participants with zero runs
            discarded).}
        \end{figure}

      \subsection{Participant task type distribution}

        Since the participants were free to choose any combination of tests,
        it's interesting to know which of the task was most frequently run.

				\begin{figure}[h!]
					\centering
					\includegraphics{figures/testsRunPerTask.pdf}
          \caption{
            Test runs distributed per task-types for all participants,
            participants that did ten or fewer and participants that did more
            than ten tests.
          }
				\end{figure}

        Overall, \textit{employee hours} was the most executed test by a wide
        margin when looking all the participants as one group and isolating the
        group consisting of participants that did ten or fewer tests. Looking at
        the outliers, participants that did sixteen or more tests, the
        difference was not as stark with a more even distribution.

				\begin{figure}[h!]
					\centering
					\includegraphics{figures/testsRunPerTaskOutliers.pdf}
          \caption{
            Detailed breakdown of task distribution for the outliers, 16 or
            more tests run, and their task distribution, multiples of identical
            distributions removed.
          }
          \label{label_testsRunPerTaskOutliers}
				\end{figure}

        Observing the outlier distribution in figure
        \ref{label_testsRunPerTaskOutliers}, there is only two \numRuns{20} runs
        when the total was four. This is due to three of the four runs having
        the exact same distribution, which is why only one of them, the last in
        the series above, was included.

        Additionally, it is interesting to note that neither of \numRuns{100},
        \numRuns{32} or the last \numRuns{20} impacts the total distribution
        since those groupings of completed tests are evenly spread among all the
        test task types.

      \newpage
      \subsection{Preferential task order}

        It is interesting to see if there are any discernible preferences
        of how a participant chooses in witch order to run the tests in their
        session. This is done by pulling out all the test runs that had a
        completion time from the database and grouping them by the order in
        which they were run by the participants.

        By furthering splitting the index-groups by task type it is possible to
        calculate the relative percentage of the n-th task being a specific
        type. The result for the standard group of fifteen or less tests shown
        below.
				\begin{figure}[ht!]
					\centering
					\includegraphics{figures/testsRunOrder.pdf}
          \caption{
            Percentage of task type grouped by the order in which they were
            run for participants that ran a total of fifteen tests or less.
          }
          \label{label_testsRunOrder}
        \end{figure}

        Since the legend in figure \ref{label_testsRunOrder} reflects the order in which the
        tasks appear in the user interface for the participants, it is apparent
        that the majority of participants choose to do the tasks in the
        presented order, top to bottom. As for the fifth test run, the majority
        choose do an extra of the last one (Team Performance), and after that,
        most participants opted to go back and do the first one again (Employee
        Hours).

        \newpage
        Given the wider range of the data coming from the outlier group (1-100)
        the visualization needs to be altered slightly. Since there is not
        enough room to have the bars side by side, they have been stacked by
        size, with the largest bar at the bottom.
				\begin{figure}[ht!]
					\centering
					\includegraphics{figures/testsRunOrderOutliers.pdf}
          \caption{
            Percentage of task type grouped by the order in which they were
            run for participants that ran a total of sixteen tests or more.
          }
				\end{figure}

        Initially the going-by-order tendency from figure \ref{label_testsRunOrder}
        holds for index one and two, but breaks down on the third, with the
        second option being the most prevalent. Interestingly, the sequential
        does appears between run five and nine, then disappears. Apart from
        that, it seems the preferable way to do thirty or more tests is to do
        them in batches.

        Exact values this figure and number of users in each group can be found
        tabulated in the appendix. \todo{Add table and reference}

%      \todo{
%        Continue exploring the data, time distribution? Which task was failed
%        the most? Are there thresholds in the variables where tasks start to
%        fail more often? Is there a most preferable order to do the tasks?
%        Which task is the most popular in the 5-run category? ...
%      }
%
      \newpage
      \subsection{Task types and success-rate}

        Next, the percentage of answered that are correct per task type are
        extracted out from the results. The data is split into two groups, one
        for the ordinary group with fifteen or less total tests, and one for
        the outliers with sixteen or more, result is displayed below.

				\begin{figure}[ht!]
					\centering
          \includegraphics{figures/testsResultsByType.pdf}
          \caption{
            Percentages of correct task answers split on ordinary and outlier
            groups.
          }
				\end{figure}

        According to the data, the difficulty of the test type in order of
        hardest to easiest is: \textit{Employee Hours}, \textit{Team
          Performance}, \textit{Team Workload} and \textit{Task Dependencies}.
        The data also shows that the outlier group have a tendency to be more
        correct, on average, than the ordinary group.

        Given that the data seems to suggest that success-rate increases with
        the number of performed tasks, the data is re-arranged to show the
        percentage of correct answer mapped against the task run index, shown
        below:

				\begin{figure}[ht!]
					\centering
          \includegraphics{figures/testsResultsByTaskIndex.pdf}
          \caption{
            Percentages of correct task answers split on ordinary and outlier
            groups.
          }
				\end{figure}

%				\begin{figure}[ht!]
%					\centering
%          \includegraphics{figures/testsResultsByTaskIndexAndTestType.pdf}
%          \caption{
%            Percentages of correct task answers split on ordinary and outlier
%            groups.
%          }
%				\end{figure}
%

      \newpage
      \subsection{Task types and completion times}

				\begin{figure}[h!]
					\centering
          \includegraphics{figures/testsTimesPerTaskType.pdf}
          \caption{
            Average and median completion times over all test runs.
          }
				\end{figure}

        \begin{figure}[h!]
          \centering
          \includegraphics{figures/testsTimesPerTaskTypeOutliers.pdf}
          \caption{
            Percentages of correct task answers split on ordinary and outlier
            groups.
          }
          \label{label_testsTimesPerTaskTypeOutliers}
        \end{figure}

        Separating into regular and outlier did not impact the result
        significantly, see figure \ref{label_testsTimesPerTaskTypeOutliers} in
        Appendix.



		\section{Discussion}

			\subsection{Possible improvements}

			\subsection{Threats to validity}


%				network latency?
%				multiple runs with same person?



	\chapter{Conclusions}

		Did it have an significant impact? Was the web the correct platform? What
		could be done better over the internet? Recording screen and voice?
		(Javascript, since it's already used, pull up some statistics?)

	% Should use consistent formatting when it comes to Names ("FirstName LastName", or "F. LastName")
	\makebibliography{report}

	%make sure we're on even page with the pop-sci
	\checkoddpage
	\ifoddpage
	\else
		 \newpage
		 \thispagestyle{empty}
		 \mbox{ }
	\fi
	%\begin{appendices}
	%\includepdf[pages={1}]{popsci/popsci.pdf}
	%\end{appendices}

  \chapter{Appendix}



\end{document}
