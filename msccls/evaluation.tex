After the test application was implemented and deployed, a link to the
application was shared on the authors personal Facebook-profile together
with an explanation of what the purpose of the test was. Three days
later, the same link was shared on the Massive internal mailing list.

\begin{figure}[h!]
  \centering
  \includegraphics{figures/deploy_link.pdf}
  \caption{Link-distribution and test milestones.}
  \label{label_milestones}
\end{figure}

Looking at the test setup and the final result, the participants can be
grouped into three categories, depending how far through the test-session
they ended up. The test structure has the following milestones:
\begin{itemize}
  \item{Getting past the initial information and consent form.}
  \item{Completing the pre-survey.}
  \item{Completing the post-survey.}
\end{itemize}

\section{Participants}

Out of the total of five days the application was live, in the first
three days there were 27 participants that got past the first milestone,
with 23 (85.2\%) of them completing the post-survey. At the third day a
link to the application was shared internally at Massive. The final
combined result over the five day span was; 101 participants getting past
the initial information, 79 (78.2\%) answered the initial survey and a
total of 74 (73.3\%) completing the post-survey.

Since there is no concrete information to work with for users that only
passed the first milestone, that group is excluded and the pre-survey and
post-survey groups will receive further analysis.

  \subsection{Age, gender-identity and completion times}

  In the pre-post-survey group, the average age is, 30.5 years, 28
  participants (32.9\%) identify as female, 54 (63.5\%) as male and 3
  (3.5\%) as other. Looking at the post-survey group, the average age of
  the participants shifts slightly, 31.1 years, and 24 (32.4\%), 47
  (63.5\%) and 3 (4\%) of the participants identify as female, male and
  other respectively.

  Looking at completion times for the post-post-survey group, the average
  time from registration to finishing the last test-question was 20 minutes
  and 56 seconds with a median of 4 minutes and 23 seconds. Additionally,
  the fastest completion time was 37 seconds with the longest completion
  time clocking in at 14 hours 15 minutes and 35 seconds.

  \subsection{Prerequisites, prior knowledge and education}

  Other attributes that were deemed interested included; how comfortable
  the participant felt using a computer, if they hade any general interest
  in user interface design and if they had studied user interface design in
  any capacity. They were also questioned on if they regularly practiced
  precise mouse movements through games and similar activities and finally
  if they have trouble distinguishing colors from each other. These
  questions were posed as a personal statement, such as, ''\textit{I feel
    comfortable using a computer}'' together with a range of selectable
  numerical options, ranging from one, \textit{strongly disagree} to five,
  \textit{strongly agree}.

%      The initial Facebook link gathered 27 recorded test runs after three
%      days. After sharing the link to the test internally on Massive, an
%      additional 74 completions of the test were recorded over a span of two
%      days. In total, 101 completions, from pre- to post-questionnaire were
%      recorded over a five day span.
%
%
%      According to the pre-questionnaire, that the average age of a participant
%      is 30 years, .



%      Where from?
%
%      How many?
%
%      Section male / female / other.
%
%      Average arg.
%
%      Average test-session.
%
%
%      \section{Participants where from?}
%
%      \section{?}

%\section{Setup}

%      \begin{figure}[h!]
%        \centering
%        \includegraphics{figures/information_setup.pdf}
%        \caption{Information gathering process.}
%      \end{figure}

\section{Dealing with varying testing hardware}\label{label_testingHardware}

  Since the tests were done remotely via the internet, only requiring a
  device capable running a standard browser, there was no standardized
  hardware-setup that the tests were executed on. In order to have the
  possibility to analyze impact of different types of hardware and group
  different results, the pre-questionnaire included questions about what
  kind of hardware the participant was using to access the tests.

  It was concluded that screen size and input method had the largest
  probability to affect the result given the design of the tests.
  The participants were asked to specify their screen size by selecting one
  of the following options: \textit{desktop, laptop, tablet} or
  \textit{mobile}. As for the input method the choices were:
  \textit{mouse, trackpad, touch} or \textit{other}, see figure
  \ref{label_preSurvey} for a capture.

%    \section{Procedure}

\section{Results}

  \todo{Write something here}

  \subsection{Pre-questionnaire -- itemizations}

    Before being able to perform the test, each participant has to fill
    out and submit a pre-questionnaire. This is done in order to get
    a rough demographical overview of the people participating in this
    study. Initially they are asked about age, used input device, type of
    screen category the test were conducted on, and what binary gender, if
    any, they identify as.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/preQuestionnaireAnswers.pdf}
      \caption{Answers for the pre-questionnaire.}
    \end{figure}

    Beginning with age, with each dot representing one answer, no
    participant was twenty or below, most of the  participants were between
    twenty-one and thirty-five, with fourteen being thirty-six and older.
    Since a large part of the participants came from MASSIVE, this seems to
    fit the mean age at the company, which is at thirty-two{\findref} at
    the time of this writing.

    It's is worth mentioning however, that the most represented single age
    was twenty-five, which is the default value set for the age input
    field. More on this specifically in \todoInsert{Link to threats to
      validity}.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/preQuestionnaireAnswersIdentify.pdf}
      \caption{Answers for the pre-questionnaire.}
    \end{figure}

    Asking people for what gender they identify as results in the largest
    group, 52 participants, identify as male. Again, as a large portion of
    participants came from MASSIVE, this is expected since the gender
    distribution in businesses related game-development tend to be weighted
    toward men\findref\findref.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/preQuestionnaireAnswersInputs.pdf}
      \caption{Answers for the pre-questionnaire.}
    \end{figure}

    Continuing the trend, most workstations at MASSIVE consist of a
    desktop PC where the main peripherals used for input are a mouse and a
    keyboard with the occasional drawing tablet mixed in.

    In general it was assumed that most of the participants were going to
    use a mouse and keyboard, with trackpads as a probable second place,
    making the turnout of nearly twenty participants using touch as their
    main input a bit of a surprise.

    Even though the assumed main input method for these tests is a keyboard
    and mouse at a stationary desktop computer, the overarching goal of the
    project, as stated earlier, is to be a pre-cursor to o more generalized
    usability testing framework. In order to align with this goal, it is
    important to explore the possibility of scaling the interface and
    related tests to different screen sizes, and analyze what, if any,
    impact is has on participant performance.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/preQuestionnaireAnswersScreen.pdf}
      \caption{Answers for the pre-questionnaire.}
    \end{figure}

    Requiring actual screen-sizes was seen as to cumbersome of a task to
    ask of the participants, especially if using devices other than a
    somewhat standard desktop monitor. Here the categories roughly equate
    to; \textit{Desktop}: 18" or larger, \textit{Laptop}: 13"-17", \textit{Tablet}:
    11"-12" and \textit{Mobile}: 10" and below.

  \subsection{Pre-questionnaire results}

    Aside from the physical setup, there are relevant knowledges and
    experience that could become interesting when analyzing free-form
    answers and test-results gathered from the participants. As an example,
    is the free-form feedback different from someone that is interested in
    usability-design? Does the level of computer literacy or experience
    with mouse-driven games impact completion times? Etcetera.

    In order to evaluate these aspects the second half of the
    pre-questionnaire included five questions on a five-point Likert scale,
    questions and answer distribution displayed below:

    \begin{figure}[h!]
      \textbf{Q1: I feel comfortable using a computer.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/preQuestionnaireAnswersOneToFive.pdf}
        \vspace{-1cm}
        \caption{Answers for the pre-questionnaire Q1.}
      \end{center}
    \end{figure}

    The clear majority of participants feel that they are very comfortable
    using a computer. Since this question is so heavily skewed towards
    \textit{Strongly agree}, it would be interesting to add an additional
    \textit{I see myself as an advanced computer user} or similar in order
    to try to separate out more divisions.

    \begin{figure}[h!]
      \textbf{Q2: I have a interest in UI-design.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/preQuestionnaireAnswersOneToFiveQ2.pdf}
        \vspace{-1cm}
        \caption{Answers for the pre-questionnaire Q2.}
      \end{center}
    \end{figure}

    Looking at the general interest in regards to user interface design,
    most participants either do not care about it, or find it somewhat to
    very interesting. If this was a non-anonymous study, it would be
    interesting to follow up on the seven participants that strongly
    disagree to this question. Mostly in order to differentiate if they do
    not like user interfaces an input method in general, or if it is more
    related to them not wanting to be involved in user interface design.

    \begin{figure}[h!]
      \textbf{Q3: I have studied UI-design.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/preQuestionnaireAnswersOneToFiveQ3.pdf}
        \vspace{-1cm}
        \caption{Answers for the pre-questionnaire Q3.}
      \end{center}
    \end{figure}

    In terms of having studied user interface design, the majority of
    participants have not. Since this is a pure self-assessment, the
    definition of what \textit{studied} means in this context is up for
    grabs. A follow up question in regards to what type of study;
    self-learnt, online-course, university etcetera would be needed clarify
    this information further.

    \begin{figure}[h!]
      \textbf{Q4: I play pointer based games (e.g. first person shooters).}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/preQuestionnaireAnswersOneToFiveQ4.pdf}
        \vspace{-1cm}
        \caption{Answers for the pre-questionnaire Q4.}
      \end{center}
    \end{figure}

    A slight majority agree, in some capacity, that they play games where
    the main form of input is pointer movements. Following this question
    with one that seeks to answer the underlying reason why disagreeing
    participants do not interact more with this kind of games could be
    interesting from a interface design standpoint. Additional questions or
    follow-up would be needed to determine if the disagreement it is a
    matter of taste, design faults, available time, or something completely
    different.

    \begin{figure}[h!]
      \textbf{Q5: I have trouble distinguishing some colors from each other.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/preQuestionnaireAnswersOneToFiveQ5.pdf}
        \vspace{-1cm}
        \caption{Answers for the pre-questionnaire Q5.}
        \vspace{-0.4cm}
      \end{center}
    \end{figure}

    All of the available tests incorporates different color pallets to
    varying degree, which makes it interesting to know if any of the
    participants have trouble making out the difference between colors.
    \vspace{-0.6cm}

  \newpage
  \subsection{Launch, participation and overall success ratio}

    The test-site went live 2020-01-24 with the link initially shared only
    through Facebook. On 2020-01-27 the link was shared on the MASSIVE
    internal mailing list, boosting the participation significantly.
    In total, \varHere{totalParticipants} participants moved past the
    initial information over a five day period.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/participantsOverTime.pdf}
      \vspace{-0.3cm}
      \caption{
        Total amount of participants that got past each of the milestones
        defined in figure \ref{label_milestones}.
      }
    \end{figure}

    In total \varHere{totalTests} tests were run, where
    \varHere{totalTestsCorrect} were answered correctly,
    \varHere{totalTestsUncompleted} never produced an answer, leaving
    \varHere{totalTestsIncorrect} incorrect answers. Looking only on the test
    runs without any user- or task-correlation, the chance that any given test run
    produces the correct answer is $\sim$\varHere{varTotalRatioSuccess}\%.

    \begin{figure}[h!]
      \centering
      \includegraphics[width=0.95\linewidth]{figures/runsOverTime.pdf}
      \vspace{-0.3cm}
      \caption{Lines representing total amount of test-runs together with
        the total of runs that produces the correct answer.}
      \vspace{-0.4cm}
    \end{figure}

  \newpage
  \subsection{Tests per user and defining outliers}

    The number of recommended test was five, which when completed, allowed
    the participant to continue to the final survey. However, there was
    nothing stopping each participant from doing more or less than five.
    By generating a histogram for the number of performed tests per
    participant it is possible to determine which of tests was most
    popular.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/testsPerUser.pdf}
      \caption{Participants grouped on how many test they performed.}
    \end{figure}

    In total, of the \varHere{totalParticipants} number of participant that
    started a session, \varHere{valNumAnyTestsRun} of them ran at least one
    test, which means \varHere{valTestNoTests} ran no tests. Retrieving and
    tabulating additional values, discarding users with no test-runs,
    produces the following table.

    \begin{figure}[h!]
      \centering
      \varHere{tablePrecentageOfUsers}
      \caption{%
        Tabulated values of test-run groups with corresponding percentage
        of total active participants.%
      }
    \end{figure}

    Participants that have run at most fifteen tests make up slightly more
    than 91\% of the total number of participants and will be the regular
    group, denoted as $\#r\leq15$. Inversely the remaining participants
    that have run sixteen or more tests in total, $\sim$9\% of the total
    amount of participant will be seen as the outlier group, denoted as
    $\#r\geq16$.

  \subsection{Test type distribution among participants}

    There are no restrictions in place in regards to how a participant
    can choose which task types to preform in what order. By grouping all
    the task-runs based on the type of the task, it is possible to see if
    any type is more popular than the others.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/testsRunPerTask.pdf}
      \caption{
        Distribution of task types among all total runs together with the
        total for the regular- and outlier-grouping respectively.
      }
    \end{figure}

    Examining the types distributed over all participants together
    with the different groupings, \textit{Employee Hours} is the most
    executed test type, regardless of categorization.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/testsRunPerTaskOutliers.pdf}
      \caption{
        Detailed breakdown of task distribution for the outliers, multiples
        of identical distributions removed.
      }
      \label{label_testsRunPerTaskOutliers}
    \end{figure}

    Taking a closer look at the distribution in the outlier group,
    $\#r\geq16$, reveals that almost all of them, six out of the total seven,
    are symmetrically split between all four task-types. Out of the sums
    [20, 20, 20, 32, 49, 100], it is only 49 that is not evenly divisible
    by four. This has the added effect that the participants in the
    outlier grouping, even though they might have ran the most total tests
    comparatively, do not impact the type distribution since a symmetrical
    distribution cancels itself out in this case.

  \subsection{Checking for preferential task order}

    As with the task-types, there are no no restrictions on in what order a
    participant can preform tasks. Grouping the tasks for each participant
    and sorting them in chronological order results in a view into what
    order each participant choose to run the different tasks.

    \begin{figure}[ht!]
      \centering
      \includegraphics{figures/testsRunOrder.pdf}
      \caption{
        Bar-graph showing the ratio of specific task-types depending on the
        the chronological order of the run.
      }
      \label{label_testsRunOrder}
    \end{figure}

    Since the legend in figure \ref{label_testsRunOrder} reflects the order
    in which the tasks appear in the user interface for participants, the
    majority choose to do the tasks in the presented order, top to bottom.
    As for the fifth test run, the majority choose do an extra of the last
    one (Team Performance), and after that, most participants opted to go
    back and do the first one again (Employee Hours).

    Given the wider range of the data coming from the outlier group (1-100)
    the visualization needs to be altered slightly. Since there is not
    enough room to have the bars side by side, they have been stacked, with
    the largest bar being at the bottom.
    \begin{figure}[ht!]
      \centering
      \includegraphics{figures/testsRunOrderOutliers.pdf}
      \caption{
        Stacked bars showing relation between task order and task type for
        the outlier group.
      }
    \end{figure}

    Initially the going-by-order tendency from figure
    \ref{label_testsRunOrder} holds for index one and two, but breaks down
    on the third, with the second option being the most prevalent.
    Interestingly, the sequential does appears between run five and nine,
    then disappears. Apart from that, it seems the preferable way to do
    thirty or more tests is to do them in batches.

    Exact values this figure and number of users in each group can be found
    tabulated in the appendix. \todo{Add table and reference}

%      \todo{
%        Continue exploring the data, time distribution? Which task was failed
%        the most? Are there thresholds in the variables where tasks start to
%        fail more often? Is there a most preferable order to do the tasks?
%        Which task is the most popular in the 5-run category? ...
%      }
%

  \subsection{Success-rates and task type}

    When completing a test-run the application marks the result as
    either correct or wrong in the underlying database. Extracting
    those answers and grouping them by the task-type makes it possible to
    compare the relative success- and failure-rates for each task-type.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/testsResultsByType.pdf}
      \caption{
        Bars showing percentages of all answers that are correct for a
        given task-type for regular and outlier groupings.
      }
    \end{figure}

    Reading the graph, the difficulty of the task-types in order of hardest
    to easiest is as follows: \textit{Employee Hours} is the hardest to get
    correct, followed by \textit{Team Performance}, \textit{Team Workload}
    and finally, the easiest to get a correct answer on, \textit{Task
      Dependencies}. According to the same data, the outlier group seems to
    be on average, more correct than the regular group.

%        Given that the data seems to suggest that success-rate increases with
%        the number of performed tasks, the data is re-arranged to show the
%        percentage of correct answer mapped against the task run index, shown
%        below:

%				\begin{figure}[ht!]
%					\centering
%          \includegraphics{figures/testsResultsByTaskIndex.pdf}
%          \caption{
%            Percentages of correct task answers split on ordinary and outlier
%            groups.
%          }
%				\end{figure}

%				\begin{figure}[ht!]
%					\centering
%          \includegraphics{figures/testsResultsByTaskIndexAndTestType.pdf}
%          \caption{
%            Percentages of correct task answers split on ordinary and outlier
%            groups.
%          }
%				\end{figure}
%
  \subsection{Completion times - Task types and distribution}

    Computing the mean and average completion time for the task-runs is
    simply a matter of gathering all the start and stop times from the
    database and perform the corresponding arithmetics. The resulting times,
    split in regular and outliers, is shown below.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/testsTimesPerTaskTypeOutliers.pdf}
      \vspace{-0.3cm}
      \caption{Average and median completion times for task-types. }
      \label{label_testsTimesPerTaskTypeOutliers}
    \end{figure}


    \textit{Employee Hours} has, according to the data, the longest average
    and median completion time of all the task types, which holds for both
    groupings. This makes sense since that the earlier analysis, looking at
    the overall success-rate for different tasks-types, points to
    \textit{Employee Hours} being the hardest task type of the four.

    By adding a few percentiles to a histogram based on the completion times for all
    tasks and participants, it is possible to determine that that 50\% of
    all tasks-runs were completed in 7 seconds or less, 90\% in 10 seconds
    or less and 95\% of all runs completed below 42 seconds.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/testsTimeGroupingsTotal.pdf}
      \caption{
        Histograms showing groupings of completion-times for all users
        rounded to nearest second.
      }
    \end{figure}

    Reusing the previous data by splitting it and creating one histogram
    each for the regular and outlier group gives a better view of how the
    completion times are distributed for each of the groups respectably,
    result shown below.

    \begin{figure}[h!]
      \centering
      \includegraphics{figures/testsTimeGroupings.pdf}
      \caption{
        Histograms showing groupings of completion-times rounded to nearest
        second for; all, regular and outliers user-groups.
      }
    \end{figure}

    For the outlier group, 50\% of all tests were completed in 3 seconds or
    less, compared to 9 seconds or less for participants in the regular
    group. This pattern is repeated for the 90th percentile, where 90\% of
    the tests in the outlier group were completed in 9 seconds or less,
    compared to 35 seconds or less in the regular group.

  \subsection{Effect of color pallet on completion times}

    \todo{Figure out a good way to calculate and visualize this.}

  \subsection{Largest impact on completion times}

    \todo{Possible multi variat analysis? Possible? Worth it?}

  \subsection{Post-survey questions}

    In order to gather feedback for evaluation and possible incorporation
    into the next design iterations, the test concludes with a second
    questionnaire with eight '1-5 questions'. These questions are ment to
    evaluate what participants thought about the setup, and if they have
    any suggestions, comments or improvements.

    \begin{figure}[h!]
      \textbf{Q1: The goal of each task was clear.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/postQuestionnaireAnswersOneToFiveQ1.pdf}
        \vspace{-1cm}
        \caption{Answers for the post-questionnaire Q1.}
      \end{center}
    \end{figure}

    One of the main goals was to only challenge the participants in the
    time it took them to complete a task and make it as easy for them every
    where else. This means that there should be as little confusion to what
    needs to be done in order to satisfy a test, and the challenge should
    come from selecting one of many well understood options. And since a
    clear majority at least agree that each task goal was clear, this goal
    was accomplished.

    \begin{figure}[h!]
      \textbf{Q2: Test-application looks good.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/postQuestionnaireAnswersOneToFiveQ2.pdf}
        \vspace{-1cm}
        \caption{Answers for the post-questionnaire Q2.}
      \end{center}
    \end{figure}

    While appreciating that the majority of participants was either
    indifferent or liked the design of the application, it was not one of
    the goals for this project. The main goal was to be usable and scaling
    well to different media sizes, not looking good.

    This reads either as the participants being nice since the tone of the
    project is personal, or the design was too polished. If the latter case
    is true, it indicates that this first iteration should have been in the
    hands of participants sooner.

    \begin{figure}[h!]
      \textbf{Q3: Use of colors helped with the tasks.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/postQuestionnaireAnswersOneToFiveQ3.pdf}
        \vspace{-1cm}
        \caption{Answers for the post-questionnaire Q3.}
      \end{center}
    \end{figure}

    Most people agreed that the color helped them preform their tasks.
    This question could be augmented with additional questions that asks
    more specifically about the perceived help. Additionally, it could be
    complemented with a permutation of runs that do not contain any color,
    in order to gather some test-data about runs without color.

    \begin{figure}[h!]
      \textbf{Q4: Amount of information was adequate.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/postQuestionnaireAnswersOneToFiveQ4.pdf}
        \vspace{-1cm}
        \caption{Answers for the post-questionnaire Q4.}
      \end{center}
    \end{figure}

    The Majority answered that the information that they were provided was
    adequate. Again, it would be very interesting to ask the participants
    that disagree what they felt was missing.

    \begin{figure}[h!]
      \textbf{Q5: Test-application is easy to to navigate.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/postQuestionnaireAnswersOneToFiveQ5.pdf}
        \vspace{-1cm}
        \caption{Answers for the post-questionnaire Q5.}
      \end{center}
    \end{figure}

    As stated earlier, participants should only need to apply them self
    when doing the actual tests. The goal is that the navigation should be
    easily traversable, which the majority of participants seem to agreed with.

    \begin{figure}[h!]
      \begin{center}
        \textbf{Q6: Appropriate choice of colors.}
        \includegraphics[width=\linewidth]{figures/postQuestionnaireAnswersOneToFiveQ6.pdf}
        \vspace{-1cm}
        \caption{Answers for the post-questionnaire Q6.}
      \end{center}
    \end{figure}

    The questionnaire shows that most participant thought that choice of
    colors were appropriate.

    \begin{figure}[h!]
      \textbf{Q7: Language used was easy to understand.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/postQuestionnaireAnswersOneToFiveQ7.pdf}
        \vspace{-1cm}
        \caption{Answers for the post-questionnaire Q7.}
      \end{center}
    \end{figure}

    A majority of participants agreed that the language used in the
    application was easy to understand, with none of the participants
    strongly disagreeing with the statement.

    \begin{figure}[h!]
      \textbf{Q8: Easy to understand what to do next.}
      \begin{center}
        \includegraphics[width=\linewidth]{figures/postQuestionnaireAnswersOneToFiveQ8.pdf}
        \vspace{-1cm}
        \caption{Answers for the post-questionnaire Q8.}
      \end{center}
    \end{figure}

    It is encouraging that most participants felt that they knew what to do
    next when performing the tests. It would of course be preferable if
    every one felt they knew what to do, but the result is encouraging.
